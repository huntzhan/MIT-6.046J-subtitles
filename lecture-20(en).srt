1
00:00:05,000 --> 00:00:09,160
So the last, we only have four more lectures left,

2
00:00:09,230 --> 00:00:12,110
and what Professor Demaine and I have decided to do

3
00:00:12,180 --> 00:00:19,050
is give two series of lectures on sort of advanced topics.

4
00:00:19,110 --> 00:00:24,480
So, today and Wednesday we're going to

5
00:00:24,560 --> 00:00:26,400
talk about parallel algorithms,

6
00:00:26,470 --> 00:00:28,850
algorithms where you have more than one processor

7
00:00:28,920 --> 00:00:32,170
whacking away on your problem.

8
00:00:32,250 --> 00:00:34,180
And this is a very hot topic right now

9
00:00:34,250 --> 00:00:37,540
because all of the chip manufacturers

10
00:00:37,610 --> 00:00:41,990
are now producing so-called multicore processors

11
00:00:42,070 --> 00:00:47,000
where you have more than one processor per chip.

12
00:00:47,070 --> 00:00:50,350
So, knowing something about that is good.

13
00:00:50,430 --> 00:00:53,190
The second topic we're going to cover

14
00:00:53,270 --> 00:00:54,810
is going to be caching,

15
00:00:54,890 --> 00:00:58,540
and how you design algorithms for systems with cache.

16
00:00:58,610 --> 00:01:00,110
Right now, we've sort of program to everything

17
00:01:00,190 --> 00:01:02,060
as if it were just a single level of memory,

18
00:01:02,120 --> 00:01:05,270
and for some problems

19
00:01:05,340 --> 00:01:07,870
that's not an entirely realistic model.

20
00:01:07,950 --> 00:01:09,410
You'd like to have some model

21
00:01:09,480 --> 00:01:11,790
for how the caching hierarchy works,

22
00:01:11,870 --> 00:01:13,660
and how you can take advantage of that.

23
00:01:13,730 --> 00:01:19,060
And there's been a lot of research in that area as well.

24
00:01:19,130 --> 00:01:23,150
So, both of those actually

25
00:01:23,220 --> 00:01:25,990
turn out to be my area of research.

26
00:01:26,060 --> 00:01:27,520
So, this is actually fun for me.

27
00:01:27,600 --> 00:01:30,410
Actually, most of it's fun anyway.

28
00:01:30,490 --> 00:01:38,000
So, today we'll talk about parallel algorithms.

29
00:01:39,360 --> 00:01:44,310
And the particular topic, it turns out that

30
00:01:44,390 --> 00:01:48,210
there are lots of models for parallel algorithms,

31
00:01:48,280 --> 00:01:49,600
and for parallelism.

32
00:01:49,670 --> 00:01:51,820
And it's one of the reasons that,

33
00:01:51,890 --> 00:01:56,670
whereas for serial algorithms,

34
00:01:56,750 --> 00:02:00,150
most people sort of have this basic model

35
00:02:00,220 --> 00:02:01,300
that we've been using.

36
00:02:01,390 --> 00:02:03,300
It's sometimes called a random access machine model,

37
00:02:03,370 --> 00:02:07,600
which is what we've been using to analyze things,

38
00:02:07,660 --> 00:02:09,350
whereas in the parallel space,

39
00:02:09,420 --> 00:02:12,410
there's just a huge number of models,

40
00:02:12,490 --> 00:02:13,940
and there is no general agreement

41
00:02:14,010 --> 00:02:16,120
on what is the best model

42
00:02:16,200 --> 00:02:17,530
because there are different machines

43
00:02:17,600 --> 00:02:20,520
that are made with different configurations, etc.

44
00:02:20,590 --> 00:02:23,740
And people haven't, sort of, agreed on,

45
00:02:23,810 --> 00:02:27,390
even how parallel machines should be organized.

46
00:02:27,470 --> 00:02:31,820
So, we're going to deal with a particular model,

47
00:02:31,880 --> 00:02:40,280
which goes under the rubric of dynamic multithreading,

48
00:02:40,350 --> 00:02:48,230
which is appropriate for the multicore machines

49
00:02:48,290 --> 00:02:52,930
that are now being built for shared memory programming.

50
00:02:53,010 --> 00:02:54,840
It's not appropriate for what's called

51
00:02:54,920 --> 00:02:57,790
distributed memory programs particularly

52
00:02:57,870 --> 00:03:01,210
because the processors are able to access things.

53
00:03:01,280 --> 00:03:04,420
And for those, you need more involved models.

54
00:03:04,480 --> 00:03:07,320
And so, let me start just by giving an example of

55
00:03:07,390 --> 00:03:10,260
how one would write something.

56
00:03:10,340 --> 00:03:13,770
I'm going to give you a program

57
00:03:13,850 --> 00:03:18,480
for calculating the nth Fibonacci number in this model.

58
00:03:18,550 --> 00:03:21,240
This is actually a really bad algorithm

59
00:03:21,320 --> 00:03:22,420
that I'm going to give you

60
00:03:22,500 --> 00:03:24,600
because it's going to be the exponential time algorithm,

61
00:03:24,670 --> 00:03:30,670
whereas we know from week one or two that you can

62
00:03:30,740 --> 00:03:34,850
calculate the nth Fibonacci number in how much time?

63
00:03:34,930 --> 00:03:39,740
Log n time. So, this is two exponentials off of

64
00:03:39,820 --> 00:03:45,090
what you should be able to get, OK, two exponentials off.

65
00:03:45,160 --> 00:03:49,490
OK, so here's the code.

66
00:04:35,550 --> 00:04:42,780
OK, so this is essentially the pseudocode we would write.

67
00:04:42,850 --> 00:04:44,930
And let me just explain a little bit about,

68
00:04:45,000 --> 00:04:47,430
we have a couple of key words here we haven't seen before:

69
00:04:47,490 --> 00:04:50,890
in particular, spawn and sync.

70
00:04:50,970 --> 00:04:56,800
OK, so spawn, this basically says that

71
00:04:56,870 --> 00:04:58,360
the subroutine that you're calling,

72
00:04:58,450 --> 00:05:00,390
you use it as a keyword before a subroutine,

73
00:05:00,490 --> 00:05:18,630
that it can execute at the same time as its parent.

74
00:05:21,530 --> 00:05:26,570
So here, what we say x equals spawn of fib of n minus one

75
00:05:26,650 --> 00:05:28,930
we immediately go onto the next statement.

76
00:05:29,020 --> 00:05:35,530
And now, while we're executing fib of n minus one,

77
00:05:35,590 --> 00:05:38,700
we can also be executing, now,

78
00:05:38,760 --> 00:05:41,740
this statement which itself will spawn something off.

79
00:05:41,810 --> 00:05:46,060
OK, and we continue, and then we hit the sync statement.

80
00:05:46,140 --> 00:05:47,730
And, what sync says is,

81
00:05:47,810 --> 00:05:58,190
wait until all children are done.

82
00:06:03,530 --> 00:06:06,450
OK, so it says once you get to this point,

83
00:06:06,530 --> 00:06:10,340
you've got to wait until everything here has completed

84
00:06:10,400 --> 00:06:13,960
before you execute the x plus y

85
00:06:14,030 --> 00:06:15,030
because otherwise you're going to

86
00:06:15,090 --> 00:06:18,330
try to execute the calculation

87
00:06:18,400 --> 00:06:20,590
of x plus y without having computed it yet.

88
00:06:20,660 --> 00:06:29,240
OK, so that's the basic structure.

89
00:06:29,320 --> 00:06:32,240
What this describes, notice in here we never said

90
00:06:32,310 --> 00:06:35,250
how many processors or anything we are running on.

91
00:06:35,330 --> 00:06:38,180
OK, so this actually is

92
00:06:38,250 --> 00:06:40,300
just describing logical parallelism

93
00:06:49,430 --> 00:06:53,300
——not the actual parallelism when we execute it.

94
00:06:53,380 --> 00:07:04,420
And so, what we need is a scheduler to determine

95
00:07:07,680 --> 00:07:25,720
how to map this dynamically unfolding execution

96
00:07:28,820 --> 00:07:35,930
onto whatever processors you have available.

97
00:07:35,990 --> 00:07:37,450
OK, and so today actually,

98
00:07:37,560 --> 00:07:41,760
we're going to talk mostly about scheduling.

99
00:07:41,830 --> 00:07:45,880
and then, next time we're going to talk about

100
00:07:45,950 --> 00:07:48,060
specific application algorithms,

101
00:07:48,120 --> 00:07:51,660
and how you analyze them.

102
00:07:51,730 --> 00:08:00,560
So you can view the actual multithreaded computation.

103
00:08:08,890 --> 00:08:12,790
If you take a look at the parallel instruction stream,

104
00:08:19,670 --> 00:08:25,660
it's just a directed acyclic graph, OK?

105
00:08:25,740 --> 00:08:28,970
So, let me show you how that works.

106
00:08:29,030 --> 00:08:31,630
So, normally when we have an instruction stream,

107
00:08:31,700 --> 00:08:33,910
I look at each instruction being executed.

108
00:08:33,980 --> 00:08:36,460
If I'm in a loop, I'm not looking at it as a loop.

109
00:08:36,540 --> 00:08:37,420
I'm just looking at

110
00:08:37,500 --> 00:08:40,780
the sequence of instructions that actually executed.

111
00:08:40,860 --> 00:08:42,960
I can do that just as a chain.

112
00:08:43,040 --> 00:08:44,710
Before I execute one instruction,

113
00:08:44,770 --> 00:08:47,080
I have to execute the one before it.

114
00:08:47,160 --> 00:08:48,330
Before I execute that,

115
00:08:48,410 --> 00:08:49,840
I've got to execute the one before it.

116
00:08:49,900 --> 00:08:51,310
At least, that's the abstraction.

117
00:08:51,380 --> 00:08:53,170
If you've studied processors,

118
00:08:53,230 --> 00:08:55,150
you know that there are a lot of tricks there

119
00:08:55,220 --> 00:08:59,080
in figuring out instruction level parallelism,

120
00:08:59,150 --> 00:09:00,120
and how you can actually

121
00:09:00,190 --> 00:09:03,420
make that serial instruction stream

122
00:09:03,490 --> 00:09:05,350
actually execute in parallel.

123
00:09:05,430 --> 00:09:07,050
But what we are going to be mostly talking about

124
00:09:07,090 --> 00:09:09,500
is the logical parallelism here,

125
00:09:09,560 --> 00:09:11,480
and what we can do in that context.

126
00:09:11,550 --> 00:09:20,670
So, in this DAG, the vertices are threads,

127
00:09:20,740 --> 00:09:33,750
which are maximal sequences of instructions

128
00:09:33,810 --> 00:09:48,500
not containing parallel control.

129
00:09:48,560 --> 00:09:50,120
And by parallel control,

130
00:09:50,190 --> 00:09:53,800
I just mean spawn, sync,

131
00:09:53,860 --> 00:09:59,360
and return from a spawned procedure.

132
00:09:59,430 --> 00:10:02,600
So, let's just mark the, so the vertices are threads.

133
00:10:02,660 --> 00:10:05,980
So, let's just mark what the vertices are here,

134
00:10:06,060 --> 00:10:09,790
OK, what the threads are here.

135
00:10:09,870 --> 00:10:14,030
So, when we enter the function here,

136
00:10:14,100 --> 00:10:18,000
we basically execute up to the point where,

137
00:10:18,070 --> 00:10:20,950
basically, here, let's call that thread A

138
00:10:21,020 --> 00:10:22,750
where we are just doing a sequential execution

139
00:10:22,810 --> 00:10:25,620
up to either returning

140
00:10:25,690 --> 00:10:30,000
or starting to do the spawn, fib of n minus one.

141
00:10:30,070 --> 00:10:31,760
So actually, thread A would include

142
00:10:31,860 --> 00:10:36,150
the calculation of n minus one right up to the point

143
00:10:36,220 --> 00:10:38,160
where you actually make the subroutine jump.

144
00:10:38,230 --> 00:10:40,130
That's thread A.

145
00:10:40,190 --> 00:10:43,020
Thread B would be the stuff that you would do,

146
00:10:43,090 --> 00:10:53,420
executing from fib of, sorry,

147
00:10:53,490 --> 00:10:56,870
B would be from the...

148
00:10:57,520 --> 00:10:58,670
We'd go up to the spawn.

149
00:10:58,750 --> 00:11:01,050
So, we've done the spawn. I'm really looking at this.

150
00:11:01,110 --> 00:11:05,310
So, B would be up to the spawn of y.

151
00:11:05,390 --> 00:11:10,460
OK, spawn of fib of n minus two to compute y,

152
00:11:10,530 --> 00:11:13,460
and then we'd have essentially an empty thread.

153
00:11:13,530 --> 00:11:17,060
So, I'll ignore that for now, but really

154
00:11:17,170 --> 00:11:20,320
then we have...after the sync up to the point

155
00:11:20,400 --> 00:11:22,040
that we get to the return of x plus y.

156
00:11:22,120 --> 00:11:24,040
So basically, we're just looking at

157
00:11:24,110 --> 00:11:27,800
maximal sequences of instructions that are all serial.

158
00:11:27,870 --> 00:11:31,110
And every time I do a parallel instruction, OK,

159
00:11:31,180 --> 00:11:34,190
a spawn or a sync, or return from it,

160
00:11:34,260 --> 00:11:36,760
that terminates the current thread.

161
00:11:36,830 --> 00:11:41,940
So we can look at that as a bunch of small threads.

162
00:11:42,020 --> 00:11:44,520
So those of you who are familiar with threads

163
00:11:44,620 --> 00:11:54,490
from Java threads, or POSIX threads, so-called P threads,

164
00:11:54,490 --> 00:11:57,330
those are sort of heavyweight static threads.

165
00:11:57,390 --> 00:12:00,480
This is a much lighter weight notion of thread,

166
00:12:00,550 --> 00:12:03,540
OK, that we are using in this model.

167
00:12:03,610 --> 00:12:09,950
OK, so these are the vertices.

168
00:12:10,030 --> 00:12:13,880
And now, let me map out a little bit how this works,

169
00:12:13,940 --> 00:12:16,110
so we can see where the edges come from.

170
00:12:16,190 --> 00:12:20,050
So, let's imagine we're executing fib of four.

171
00:12:22,540 --> 00:12:27,940
So, I'm going to draw a horizontal oval.

172
00:12:28,020 --> 00:12:31,520
That's going to correspond to the procedure execution.

173
00:12:31,600 --> 00:12:32,970
And, in this procedure,

174
00:12:33,050 --> 00:12:36,550
there are essentially three threads.

175
00:12:36,620 --> 00:12:37,950
We start out with A,

176
00:12:38,010 --> 00:12:46,810
so this is our initial thread is this guy here.

177
00:12:46,890 --> 00:12:50,890
And then, when he executes a spawn,

178
00:12:50,960 --> 00:12:54,090
OK, he's going to execute a spawn,

179
00:12:54,170 --> 00:12:55,520
we are going to create a new procedure,

180
00:12:55,590 --> 00:13:00,780
and he's going to execute a new A

181
00:13:00,830 --> 00:13:03,480
recursively within that procedure.

182
00:13:03,540 --> 00:13:06,890
But at the same time, we're also going to be,

183
00:13:06,960 --> 00:13:10,620
now, allowed to go on and execute B in the parent,

184
00:13:10,700 --> 00:13:14,480
we have parallelism here when I do a spawn.

185
00:13:14,550 --> 00:13:18,020
OK, and so there's an edge here.

186
00:13:18,090 --> 00:13:20,920
This edge we are going to call a spawn edge,

187
00:13:20,990 --> 00:13:25,460
and this is called a continuation edge

188
00:13:25,540 --> 00:13:29,670
because it's just simply

189
00:13:29,750 --> 00:13:33,170
continuing the procedure execution.

190
00:13:33,240 --> 00:13:41,080
OK, now at this point, this guy, we now have two things

191
00:13:41,150 --> 00:13:42,690
that can execute at the same time.

192
00:13:42,760 --> 00:13:43,770
Once I've executed A,

193
00:13:43,840 --> 00:13:46,330
I now have two things that can execute.

194
00:13:46,390 --> 00:13:48,450
OK, so this one,

195
00:13:48,530 --> 00:13:54,040
for example, may spawn another thread here.

196
00:13:54,110 --> 00:13:56,030
Oh, so this is fib of three, right?

197
00:13:56,090 --> 00:13:59,890
And this is now fib of two.

198
00:13:59,950 --> 00:14:04,360
OK, so he spawns another guy here,

199
00:14:04,440 --> 00:14:08,900
and simultaneously, he can go on and execute B here,

200
00:14:08,980 --> 00:14:12,420
OK, with a continued edge.

201
00:14:12,490 --> 00:14:15,260
And B, in fact, can also spawn at this point.

202
00:14:21,290 --> 00:14:25,770
OK, and this is now fib of two also.

203
00:14:30,830 --> 00:14:42,850
And now, at this point, we can't execute C yet here

204
00:14:42,940 --> 00:14:46,390
even though I've spawned things off.

205
00:14:46,470 --> 00:14:50,030
And the reason is because C won't execute

206
00:14:50,100 --> 00:14:53,750
until we've executed the sync statement,

207
00:14:53,830 --> 00:14:59,150
which can't occur until A and B have both been executed,

208
00:14:59,220 --> 00:15:03,530
OK? So, he just sort of sits there waiting,

209
00:15:03,600 --> 00:15:07,070
OK, and a scheduler can't try to schedule him.

210
00:15:07,140 --> 00:15:12,130
Or if he does, then nothing's going to happen here, OK?

211
00:15:12,200 --> 00:15:15,310
So, we can go on.

212
00:15:15,390 --> 00:15:23,930
Let's see, here we could call fib of one.

213
00:15:24,000 --> 00:15:27,270
The fib of one is only going to

214
00:15:27,350 --> 00:15:29,920
execute an A statement here.

215
00:15:30,000 --> 00:15:33,310
OK, of course it can't continue here

216
00:15:33,380 --> 00:15:35,630
because A is the only thing,

217
00:15:35,700 --> 00:15:37,960
when I execute fib of one, if we look at the code,

218
00:15:38,030 --> 00:15:43,020
it never executes B or C. OK, and similarly here,

219
00:15:43,100 --> 00:15:56,000
this guy here to do fib of one.

220
00:15:56,070 --> 00:16:00,850
And this guy, I guess, could execute a here of fib of one

221
00:16:07,950 --> 00:16:18,410
OK, and maybe now this guy calls his another fib of one,

222
00:16:18,490 --> 00:16:23,440
and this guy does another one.

223
00:16:23,520 --> 00:16:27,020
This is going to be fib of zero, right?

224
00:16:27,100 --> 00:16:33,730
I keep drawing that arrow to the wrong place.

225
00:16:33,810 --> 00:16:38,470
And now, once these guys return, well,

226
00:16:38,540 --> 00:16:42,490
let's say these guys return here, I can now execute C.

227
00:16:42,560 --> 00:16:45,520
But I can't execute with them

228
00:16:45,600 --> 00:16:48,820
until both of these guys are done, and that guy is done.

229
00:16:48,900 --> 00:16:50,660
So, you see that we get

230
00:16:50,760 --> 00:16:54,320
a synchronization point here before executing C.

231
00:16:54,390 --> 00:16:58,860
And then, similarly here,

232
00:16:58,940 --> 00:17:00,960
now that we've executed this and this,

233
00:17:01,030 --> 00:17:02,940
we can now execute this guy here.

234
00:17:03,020 --> 00:17:10,600
And so, those returns go to there.

235
00:17:10,670 --> 00:17:15,090
Likewise here, this guy can now execute his C,

236
00:17:15,160 --> 00:17:19,700
and now once both of those are done,

237
00:17:19,760 --> 00:17:22,810
we can execute this guy here.

238
00:17:22,870 --> 00:17:27,830
And then we are done.

239
00:17:27,890 --> 00:17:33,570
This is our final thread.

240
00:17:33,640 --> 00:17:38,130
So, I should have labeled also that

241
00:17:38,200 --> 00:17:39,620
when I get one of these guys here,

242
00:17:39,690 --> 00:17:43,450
that's a return edge.

243
00:17:43,530 --> 00:17:56,900
So, the three types of edges are spawn, return,

244
00:17:56,980 --> 00:18:07,610
and continuation. OK, and by describing it in this way,

245
00:18:07,680 --> 00:18:10,800
I essentially get a DAG that unfolds.

246
00:18:10,870 --> 00:18:14,030
So, rather than having just a serial execution trace,

247
00:18:14,100 --> 00:18:18,200
I get something where I have

248
00:18:18,270 --> 00:18:19,600
still some serial dependencies that

249
00:18:19,670 --> 00:18:22,650
some things has to be done before other things

250
00:18:22,720 --> 00:18:26,210
but there are also things that can be done at the same time.

251
00:18:26,320 --> 00:18:30,670
So how are we doing? Yeah, question?

252
00:18:30,740 --> 00:18:36,080
[Student]:......

253
00:18:36,140 --> 00:18:39,080
[Professor]:Is every spawn were covered by a sync,

254
00:18:39,160 --> 00:18:41,610
effectively, yeah, yeah, effectively.

255
00:18:41,670 --> 00:18:44,250
There's actually a null thread that gets executed in there,

256
00:18:44,330 --> 00:18:45,720
which I hadn't bothered to show.

257
00:18:45,790 --> 00:18:49,640
But yes, basically you would then not have any parallelism,

258
00:18:49,710 --> 00:18:51,370
OK, because you would spawn it off,

259
00:18:51,430 --> 00:18:53,320
but then you're not doing anything in the parent.

260
00:18:53,380 --> 00:18:55,820
So it's pretty much the same,

261
00:18:55,900 --> 00:18:59,170
yeah, as if it had executed serially.

262
00:18:59,250 --> 00:19:03,190
Yep, OK, so you can see that basically what we had here

263
00:19:03,270 --> 00:19:06,850
in some sense is a DAG embedded in a tree.

264
00:19:06,910 --> 00:19:10,810
OK, so you have a tree

265
00:19:10,870 --> 00:19:12,400
that's sort of the procedure structure,

266
00:19:12,470 --> 00:19:13,980
but in there you have a DAG,

267
00:19:14,040 --> 00:19:16,320
and that DAG can actually get to be pretty complicated.

268
00:19:16,400 --> 00:19:19,470
OK, now what I want to do is now that

269
00:19:19,540 --> 00:19:21,560
we understand that we've got an underlying DAG,

270
00:19:21,620 --> 00:19:24,260
I want to switch to trying to study

271
00:19:24,330 --> 00:19:29,100
the performance attributes of a particular DAG execution,

272
00:19:29,190 --> 00:19:32,050
so looking at performance measures.

273
00:19:43,230 --> 00:19:47,340
So, the notation that we'll use is

274
00:19:47,410 --> 00:19:50,700
we'll let T_P be the running time

275
00:19:50,770 --> 00:20:00,860
of whatever our computation is on P processors.

276
00:20:03,540 --> 00:20:06,460
OK, so, T_P is,

277
00:20:06,540 --> 00:20:09,000
how long does it take to execute this on P processors

278
00:20:09,060 --> 00:20:10,590
Now, in general, this is not going to be

279
00:20:10,700 --> 00:20:13,650
just a particular number,

280
00:20:13,750 --> 00:20:17,980
because I can have different scheduling disciplines

281
00:20:18,040 --> 00:20:22,690
would lead me to get numbers for T_P, OK?

282
00:20:22,760 --> 00:20:24,350
But when we talk about the running time,

283
00:20:24,420 --> 00:20:25,920
we'll still sort of use this notation,

284
00:20:26,000 --> 00:20:29,350
and I'll try to be careful as we go through to make sure

285
00:20:29,430 --> 00:20:31,640
that there's no confusion about what that means in context.

286
00:20:31,720 --> 00:20:33,650
There are a couple of them,

287
00:20:33,730 --> 00:20:35,710
though, which are fairly well defined.

288
00:20:35,790 --> 00:20:38,890
One is...Based on this.

289
00:20:38,960 --> 00:20:43,040
One is T_1. So, T_1 is the running time on one processor.

290
00:20:43,100 --> 00:20:46,190
OK, so if I were to execute

291
00:20:46,250 --> 00:20:48,890
this on one processor, you can imagine

292
00:20:48,960 --> 00:20:51,670
it's just as if I had just got rid of the spawn and syncs

293
00:20:51,750 --> 00:20:54,480
and everything, and just executed it.

294
00:20:54,550 --> 00:20:57,570
That will give me a particular running time.

295
00:20:57,640 --> 00:21:02,140
We call that running time on one processor the work.

296
00:21:02,220 --> 00:21:06,770
It's essentially the serial time.

297
00:21:09,580 --> 00:21:12,400
OK, so when we talk about the work of a computation,

298
00:21:12,470 --> 00:21:14,670
we just been essentially a serial running time.

299
00:21:14,740 --> 00:21:19,490
OK, the other measure that ends up being interesting

300
00:21:19,550 --> 00:21:22,300
is what we call T infinity.

301
00:21:22,380 --> 00:21:27,950
OK, and this is the critical pathlength,

302
00:21:33,800 --> 00:21:43,850
OK, which is essentially the longest path in the DAG.

303
00:21:43,930 --> 00:21:45,290
So, for example,

304
00:21:45,370 --> 00:21:52,220
if we look at the fib of four in this example,

305
00:21:52,290 --> 00:21:55,380
it has T of one equal to,

306
00:21:55,460 --> 00:21:58,180
so let's assume we have unit time threads.

307
00:21:58,260 --> 00:22:00,290
I know they're not unit time, but let's just imagine,

308
00:22:00,370 --> 00:22:01,870
for the purposes of understanding this,

309
00:22:01,940 --> 00:22:07,350
that every thread costs me one unit of time to execute.

310
00:22:07,420 --> 00:22:12,800
What would be the work of this particular computation?

311
00:22:12,880 --> 00:22:18,640
17, right, OK,

312
00:22:18,700 --> 00:22:22,740
because all we do is just add up three, six,

313
00:22:22,810 --> 00:22:28,120
nine, 12, 13, 14, 15, 16, 17.

314
00:22:28,200 --> 00:22:29,880
So, the work is 17 in this case

315
00:22:29,950 --> 00:22:31,270
if it were unit time threads.

316
00:22:31,340 --> 00:22:33,700
In general, you would add up how many

317
00:22:33,710 --> 00:22:35,610
instructions or whatever were in there.

318
00:22:35,620 --> 00:22:40,750
OK, and then T infinity is the longest path.

319
00:22:40,820 --> 00:22:42,330
So, this is the longest sequence.

320
00:22:42,400 --> 00:22:44,580
It's like, if you had an infinite number of processors,

321
00:22:44,660 --> 00:22:48,220
you still can't just do everything at once

322
00:22:48,280 --> 00:22:50,600
because some things have to come before other things.

323
00:22:50,660 --> 00:22:53,500
But if you had an infinite number of processors,

324
00:22:53,580 --> 00:22:54,800
as many processors as you want,

325
00:22:54,870 --> 00:22:57,490
what's the fastest you could possibly execute this?

326
00:23:01,520 --> 00:23:06,230
A little trickier. Seven?

327
00:23:06,290 --> 00:23:09,030
So, what's your seven?

328
00:23:09,100 --> 00:23:11,680
[Student]:......

329
00:23:11,740 --> 00:23:21,600
So, one, two, three, four, five, six, seven,

330
00:23:21,670 --> 00:23:25,800
eight, yeah, eight is the longest path.

331
00:23:25,880 --> 00:23:32,810
So, the work and the critical path length, as we'll see,

332
00:23:32,880 --> 00:23:36,400
are key attributes of any computation.

333
00:23:36,480 --> 00:23:42,130
And abstractly, and this is just for those

334
00:23:42,200 --> 00:23:46,280
if they're unit time threads.

335
00:23:51,460 --> 00:23:56,030
OK, so we can use these two measures

336
00:23:56,100 --> 00:24:00,650
to derive lower bounds on T_P

337
00:24:00,680 --> 00:24:06,700
for P that fall between one and infinity,

338
00:24:06,790 --> 00:24:08,200
Ok?

339
00:24:18,590 --> 00:24:22,090
OK, so the first lower bound we can derive is that

340
00:24:22,170 --> 00:24:26,750
T_P has got to be at least T_1 over P.

341
00:24:26,820 --> 00:24:38,110
OK, so why is that a lower bound? Yeah?

342
00:24:38,160 --> 00:24:45,430
[Student]：......

343
00:24:45,490 --> 00:24:47,500
[Professor]：But if I have P processors,

344
00:24:47,550 --> 00:24:54,260
[Student]：......

345
00:24:54,340 --> 00:24:56,460
[Professor]：and, OK, and why would I have this lower bound?

346
00:24:56,530 --> 00:25:02,310
[Student]：......

347
00:25:02,390 --> 00:25:04,360
[Professor]：OK, yeah, you've got the right idea.

348
00:25:04,440 --> 00:25:08,240
So, but can we be a little bit more articulate about it?

349
00:25:08,310 --> 00:25:11,420
So, that's right, so you want to use all of processors.

350
00:25:11,500 --> 00:25:13,720
If you could use all of processors,

351
00:25:13,790 --> 00:25:16,520
why couldn't I use all the processors,

352
00:25:16,600 --> 00:25:18,920
though, and have T_P be less than this?

353
00:25:18,990 --> 00:25:25,620
Why does it have to be at least as big as T_1 over P?

354
00:25:25,690 --> 00:25:30,450
I'm just asking for a little more precision in the answer.

355
00:25:30,530 --> 00:25:32,210
You've got exactly the right idea,

356
00:25:32,280 --> 00:25:34,500
but I need a little more precision

357
00:25:34,560 --> 00:25:35,840
if we're going to persuade the rest of the class

358
00:25:35,920 --> 00:25:40,230
that this is the lower bound.

359
00:25:40,300 --> 00:25:41,290
Yeah?

360
00:25:41,360 --> 00:25:48,750
[Student]：......

361
00:25:48,820 --> 00:25:51,260
[Professor]：Yeah, that's another way of looking at it.

362
00:25:51,330 --> 00:25:54,610
If you were to serialize the computation, OK,

363
00:25:54,680 --> 00:25:57,500
so whatever things you execute on each step,

364
00:25:57,590 --> 00:26:00,230
you do P of them, and so if you serialized it,

365
00:26:00,310 --> 00:26:02,660
somehow then it would take you P steps

366
00:26:02,720 --> 00:26:05,200
to execute one step of a P way,

367
00:26:05,260 --> 00:26:08,690
a machine with P processors.

368
00:26:08,770 --> 00:26:11,170
So then, OK, yeah?

369
00:26:11,260 --> 00:26:13,330
OK, maybe a little more precise.

370
00:26:13,400 --> 00:26:14,350
There?

371
00:26:14,460 --> 00:26:27,130
[Student]：......

372
00:26:27,200 --> 00:26:31,290
[Professor]：Yeah, good, so let me just state this a little bit.

373
00:26:31,350 --> 00:26:35,060
So, P processors, so what are we relying on?

374
00:26:35,140 --> 00:26:44,720
P processors can do, at most, P work in one step,

375
00:26:44,800 --> 00:26:50,140
right? So, in one step they do at most P work.

376
00:26:50,290 --> 00:26:53,730
They can't do more than P work.

377
00:26:53,790 --> 00:26:57,940
And so, if they can do at most P work in one step,

378
00:26:58,000 --> 00:27:01,670
then if the number of steps was, in fact,

379
00:27:01,730 --> 00:27:03,050
less than T_1 over P,

380
00:27:03,110 --> 00:27:11,540
they would be able to do more than T_1 work in P steps.

381
00:27:11,620 --> 00:27:13,440
And, there's only T_1 work to be done.

382
00:27:13,520 --> 00:27:17,440
OK, I just stated that

383
00:27:17,520 --> 00:27:20,530
almost as badly as all the responses I got.

384
00:27:20,600 --> 00:27:24,230
OK, P processors can do,

385
00:27:24,300 --> 00:27:27,960
at most, P work in one step, right?

386
00:27:28,040 --> 00:27:30,990
So, if there's T_1 work to be done,

387
00:27:31,070 --> 00:27:32,900
the number of steps is

388
00:27:32,990 --> 00:27:37,270
going to be at least T_1 over P, OK?

389
00:27:37,340 --> 00:27:41,030
There we go. OK, it wasn't that hard.

390
00:27:41,110 --> 00:27:42,590
It's just like, I've got a certain amount of,

391
00:27:42,660 --> 00:27:43,800
I've got T_1 work to do.

392
00:27:43,910 --> 00:27:46,590
I can knock off, at most, P on every step.

393
00:27:46,660 --> 00:27:49,470
How many steps? Just divide.

394
00:27:49,580 --> 00:27:52,730
OK, so it's going to have to be at least that amount.

395
00:27:52,800 --> 00:27:54,800
OK, good.

396
00:27:54,870 --> 00:27:58,050
The other lower bound is T_P

397
00:27:58,130 --> 00:28:00,270
is greater than or equal to T infinity.

398
00:28:00,350 --> 00:28:10,350
Somebody explain to me why that might be true. Yeah?

399
00:28:10,420 --> 00:28:14,360
[Student]：......

400
00:28:14,390 --> 00:28:16,220
[Prof]:Yeah, if you have an infinite number of processors

401
00:28:16,290 --> 00:28:19,280
You also have... you have P.

402
00:28:19,360 --> 00:28:23,400
So if you could do it in a certain amount of time with P,

403
00:28:23,480 --> 00:28:24,880
you can certainly do it in that time

404
00:28:24,960 --> 00:28:26,390
with an infinite number of processors.

405
00:28:26,470 --> 00:28:28,170
OK, this is in this model where, you know,

406
00:28:28,250 --> 00:28:30,200
there is lots of stuff that this model doesn't model

407
00:28:30,220 --> 00:28:33,550
like communication costs and interference,

408
00:28:33,630 --> 00:28:35,730
and all sorts of things. But it is simple model,

409
00:28:35,740 --> 00:28:38,600
which actually in practice works out pretty well,

410
00:28:41,420 --> 00:28:43,780
You're not going to be able

411
00:28:43,850 --> 00:28:47,040
to do more work with P processors

412
00:28:47,110 --> 00:28:50,790
than you are with an infinite number of processors.

413
00:29:03,680 --> 00:29:13,900
OK, so those are helpful bounds to understand

414
00:29:13,960 --> 00:29:18,900
when we are trying to make something go faster,

415
00:29:18,980 --> 00:29:22,820
it's nice to know what you could possibly hope to achieve,

416
00:29:22,900 --> 00:29:26,780
OK, as opposed to beating your head against a wall,

417
00:29:26,850 --> 00:29:29,130
how come I can't get it to go much faster?

418
00:29:29,190 --> 00:29:33,250
Maybe it's because one of these lower bounds is operating.

419
00:29:33,330 --> 00:29:37,570
OK, well, we're interested in how fast we can go.

420
00:29:37,650 --> 00:29:42,780
That's the main reason for using multiple processors

421
00:29:42,860 --> 00:29:44,120
is you hope you're going to go faster

422
00:29:44,200 --> 00:29:45,920
than you could with one processor.

423
00:29:46,000 --> 00:29:51,250
So, we define T_1 over T_P to be

424
00:29:51,330 --> 00:30:00,290
the speedup on P processors.

425
00:30:00,370 --> 00:30:05,950
OK, so we say, how much faster is it

426
00:30:06,010 --> 00:30:08,400
on P processors than on one processor?

427
00:30:08,470 --> 00:30:19,040
OK, that's the speed up. If T_1 over T_P is order P,

428
00:30:19,110 --> 00:30:23,850
we say that it's linear speedup.

429
00:30:23,920 --> 00:30:30,060
OK, in other words, why?

430
00:30:30,130 --> 00:30:36,370
Because that says that it means that

431
00:30:36,440 --> 00:30:39,490
if I've thrown P processors at the job

432
00:30:39,550 --> 00:30:47,010
I'm going to get a speedup that's proportional to P.

433
00:30:47,090 --> 00:30:51,290
OK, so when I throw P processors at the job

434
00:30:51,360 --> 00:30:54,270
and I get T_P, if that's order P,

435
00:30:54,340 --> 00:30:55,540
that means that in some sense

436
00:30:55,610 --> 00:30:57,250
my processors each contributed

437
00:30:57,330 --> 00:31:02,220
within a constant factor its full measure of support.

438
00:31:02,280 --> 00:31:04,570
If this, in fact, were equal to P,

439
00:31:04,630 --> 00:31:06,630
we'd call that perfect linear speedup.

440
00:31:06,710 --> 00:31:13,070
OK, so but here we're looking at giving ourselves,

441
00:31:13,150 --> 00:31:14,520
for theoretical purposes,

442
00:31:14,600 --> 00:31:18,700
a little bit of a constant buffer here, perhaps.

443
00:31:18,770 --> 00:31:27,380
If T_1 over T_P is greater than P,

444
00:31:27,450 --> 00:31:37,260
we call that super linear speedup.

445
00:31:38,510 --> 00:31:42,140
OK, so can somebody tell me,

446
00:31:42,200 --> 00:31:45,820
when can I get super linear speedup?

447
00:31:54,030 --> 00:31:58,030
When can I get super linear speed up?

448
00:31:58,100 --> 00:32:00,470
Never. OK, why never?

449
00:32:00,520 --> 00:32:03,500
[Student]：......

450
00:32:03,570 --> 00:32:06,140
Yeah, if we buy these lower bounds,

451
00:32:06,220 --> 00:32:07,560
the first lower bound there,

452
00:32:07,620 --> 00:32:11,520
it is T_P is greater than or equal to T_1 over P.

453
00:32:11,600 --> 00:32:14,740
And, if I just take T_1 over T_P,

454
00:32:14,810 --> 00:32:17,020
that says it's less than or equal to P.

455
00:32:17,090 --> 00:32:19,590
So, this is never,

456
00:32:19,660 --> 00:32:22,460
OK, not possible in this model.

457
00:32:26,490 --> 00:32:29,510
OK, there are other models

458
00:32:29,590 --> 00:32:31,660
where it is possible to get super linear speed up

459
00:32:31,730 --> 00:32:34,730
due to caching effects, and things of that nature.

460
00:32:34,790 --> 00:32:36,480
But in this simple model that we are dealing with,

461
00:32:36,550 --> 00:32:39,400
it's not possible to get super linear speedup.

462
00:32:39,480 --> 00:32:45,080
OK, not possible.

463
00:32:45,150 --> 00:32:48,660
Now, the maximum possible speedup,

464
00:32:56,630 --> 00:33:00,980
given some amount of work

465
00:33:01,060 --> 00:33:08,300
and critical path length is what?

466
00:33:08,380 --> 00:33:10,030
What's the maximum possible speed up

467
00:33:10,120 --> 00:33:17,820
I could get over any number of processors?

468
00:33:17,890 --> 00:33:21,360
What's the maximum I could possibly get?

469
00:33:27,210 --> 00:33:30,940
No, I'm saying, no matter how many processors,

470
00:33:31,020 --> 00:33:35,180
what's the most speedup that I could get?

471
00:33:38,040 --> 00:33:46,920
T_1 over T infinity, because this is the,

472
00:33:46,980 --> 00:33:50,960
so T_1 over T infinity is the maximum I could possibly get.

473
00:33:51,030 --> 00:33:54,270
If I threw an infinite number of processors at the problem

474
00:33:54,330 --> 00:33:56,380
that's going to give me my biggest speedup.

475
00:33:56,460 --> 00:33:59,100
OK, and we call that the parallelism.

476
00:34:02,770 --> 00:34:05,390
OK, so that's defined to be the parallelism.

477
00:34:05,460 --> 00:34:09,190
So the parallelism of the particular algorithm is

478
00:34:09,270 --> 00:34:15,120
essentially the work divided by the critical path length.

479
00:34:15,210 --> 00:34:17,460
Another way of viewing it is that

480
00:34:17,540 --> 00:34:27,360
this is the average amount of work

481
00:34:27,430 --> 00:34:37,460
that can be done in parallel

482
00:34:40,690 --> 00:34:49,580
along each step of the critical path.

483
00:34:49,650 --> 00:34:56,670
And, we denote it often by P bar.

484
00:34:56,740 --> 00:34:59,940
So, do not get confused.

485
00:35:00,010 --> 00:35:04,330
P bar does not have anything to do with P at some level.

486
00:35:04,410 --> 00:35:05,890
OK, P is going to be a

487
00:35:05,910 --> 00:35:07,820
certain number of processors you're running.

488
00:35:07,890 --> 00:35:10,260
P bar is defined just in terms of

489
00:35:10,340 --> 00:35:12,110
the computation you're executing,

490
00:35:12,180 --> 00:35:15,200
not in terms of the machine you're running it on.

491
00:35:15,280 --> 00:35:20,200
OK, it's just the average amount of work

492
00:35:20,270 --> 00:35:21,580
that can be done in parallel

493
00:35:21,640 --> 00:35:24,080
along each step of the critical path.

494
00:35:24,150 --> 00:35:28,980
OK, questions so far?

495
00:35:29,040 --> 00:35:31,590
So mostly we're just doing definitions so far.

496
00:35:31,650 --> 00:35:35,540
OK, now we get into,

497
00:35:35,620 --> 00:35:40,260
OK, so it's helpful to know what the parallelism is,

498
00:35:40,330 --> 00:35:44,940
because the parallelism is going to,

499
00:35:45,010 --> 00:35:47,580
there's no real point in trying to

500
00:35:47,640 --> 00:35:50,740
get speed up bigger than the parallelism.

501
00:35:50,810 --> 00:35:54,570
OK, so if you are given a particular computation,

502
00:35:54,640 --> 00:35:56,980
you'll be able to say, oh, it doesn't go any faster.

503
00:35:57,050 --> 00:35:58,880
You're throwing more processors at it.

504
00:35:58,950 --> 00:36:00,540
Why is it that going any faster?

505
00:36:00,620 --> 00:36:03,960
And the answer could be, no more parallelism.

506
00:36:06,030 --> 00:36:12,590
OK, let's see what I want to,

507
00:36:12,860 --> 00:36:19,190
yeah, I think we can raise the example here.

508
00:36:19,270 --> 00:36:21,740
We'll talk more about this model.

509
00:36:21,820 --> 00:36:24,530
Mostly, now, we're going to just talk about DAG's.

510
00:36:24,610 --> 00:36:29,630
So, we'll talk about the programming model next time.

511
00:36:39,310 --> 00:36:42,730
So, let's talk about scheduling.

512
00:36:42,810 --> 00:36:43,910
The goal of scheduler is

513
00:36:43,980 --> 00:36:51,240
to map the computation to P processors.

514
00:36:51,300 --> 00:36:58,650
And this is typically done by a runtime system,

515
00:36:58,730 --> 00:37:04,110
which, if you will, is an algorithm that

516
00:37:04,170 --> 00:37:08,960
is running underneath the language layer that I showed you.

517
00:37:09,050 --> 00:37:11,260
OK, so the programmer designs

518
00:37:11,340 --> 00:37:14,030
an algorithm using spawns, and syncs, and so forth.

519
00:37:14,090 --> 00:37:18,770
Then, underneath that, there's an algorithm that has to

520
00:37:18,850 --> 00:37:21,280
actually map that executing program

521
00:37:21,350 --> 00:37:24,820
onto the processors of the machine as it executes.

522
00:37:24,900 --> 00:37:26,890
And that's the scheduler.

523
00:37:26,950 --> 00:37:30,600
OK, so it's done by the language runtime system, typically.

524
00:37:30,670 --> 00:37:35,040
OK, so it turns out that online schedulers,

525
00:37:37,120 --> 00:37:44,330
let me just say they're complex.

526
00:37:44,410 --> 00:37:47,150
OK, they're not necessarily easy things to build.

527
00:37:47,210 --> 00:37:49,330
OK, they're not too bad actually.

528
00:37:49,400 --> 00:37:51,660
But, we are not going to go there

529
00:37:51,730 --> 00:37:54,270
because we only have two lectures to do this.

530
00:37:54,340 --> 00:37:55,990
Instead, we're going to do is

531
00:37:56,070 --> 00:38:07,870
we'll illustrate the ideas using off-line scheduling.

532
00:38:14,200 --> 00:38:17,060
OK, so you'll get an idea

533
00:38:17,140 --> 00:38:21,560
out of this for what a scheduler does,

534
00:38:21,650 --> 00:38:25,150
and it turns out that doing these things online is

535
00:38:25,210 --> 00:38:28,230
another level of complexity beyond that.

536
00:38:28,290 --> 00:38:30,080
And typically, the online schedulers

537
00:38:30,110 --> 00:38:33,010
that are good these days, are randomized schedulers.

538
00:38:33,090 --> 00:38:35,130
And they have very strong proofs

539
00:38:35,200 --> 00:38:37,380
of their ability to perform.

540
00:38:37,460 --> 00:38:38,940
But we're not going to go there.

541
00:38:39,010 --> 00:38:41,430
We'll keep it simple.

542
00:38:51,760 --> 00:38:53,760
And in particular, we're going to look at

543
00:38:53,820 --> 00:38:57,330
a particular type of scheduler called a greedy scheduler.

544
00:39:03,100 --> 00:39:07,030
So, if you have a DAG to execute,

545
00:39:07,100 --> 00:39:10,490
so the basic rules of the scheduler is

546
00:39:10,570 --> 00:39:15,030
you can't execute a node until all of the nodes

547
00:39:15,100 --> 00:39:19,700
that precede it in the DAG have executed.

548
00:39:19,780 --> 00:39:24,590
OK, so you've got to wait until everything is executed.

549
00:39:24,660 --> 00:39:29,210
So, a greedy scheduler, what it says is let's just

550
00:39:29,280 --> 00:39:34,150
try to do as much as possible on every step, OK?

551
00:39:47,080 --> 00:39:51,660
In other words, it says I'm never going to try to

552
00:39:51,740 --> 00:39:55,150
guess that it's worthwhile delaying doing something.

553
00:39:55,220 --> 00:39:57,110
If I could do something now, I'm going to do it.

554
00:39:57,190 --> 00:40:01,020
And so, each step is going to

555
00:40:01,090 --> 00:40:04,000
correspond to be one of two types.

556
00:40:04,070 --> 00:40:08,300
The first type is what we'll call a complete step.

557
00:40:08,370 --> 00:40:12,060
And this is a step in which

558
00:40:12,140 --> 00:40:22,290
there are at least P threads ready to run.

559
00:40:22,350 --> 00:40:27,590
And, I'm executing on P processors.

560
00:40:31,310 --> 00:40:36,350
There are at least P threads ready to run.

561
00:40:36,410 --> 00:40:38,660
So, what's a greedy strategy here?

562
00:40:38,730 --> 00:40:45,070
I've got P processors. I've got at least P threads.

563
00:40:45,140 --> 00:40:52,830
Run any P. Yeah, first P would be

564
00:40:52,900 --> 00:40:54,810
if you had a notion of ordering.

565
00:40:54,870 --> 00:40:57,020
That would be perfectly reasonable.

566
00:40:57,100 --> 00:40:59,850
Here, we are just going to execute any P.

567
00:41:02,770 --> 00:41:06,830
We might make a mistake there,

568
00:41:06,910 --> 00:41:09,270
because there may be a particular one that

569
00:41:09,340 --> 00:41:11,370
if we execute now,

570
00:41:11,440 --> 00:41:14,070
that'll enable more parallelism later on.

571
00:41:14,140 --> 00:41:17,130
We might not execute that one. We don't know.

572
00:41:17,200 --> 00:41:19,530
OK, but basically, what we're going to do

573
00:41:19,610 --> 00:41:22,980
is just execute any P willy-nilly. So, there's some,

574
00:41:23,060 --> 00:41:26,650
if you will, non-determinism in this step here

575
00:41:26,730 --> 00:41:28,410
because which one you execute

576
00:41:28,470 --> 00:41:31,870
may or may not be a good choice.

577
00:41:31,950 --> 00:41:35,850
OK, the second type of step we're going to have

578
00:41:35,930 --> 00:41:44,540
is an incomplete step. And this is a situation

579
00:41:44,610 --> 00:41:47,650
where we have fewer than P threads ready to run.

580
00:41:54,350 --> 00:41:57,790
So, what's our strategy there?

581
00:42:01,230 --> 00:42:06,450
Execute all of them. OK, if it's greedy,

582
00:42:06,530 --> 00:42:09,430
no point in not executing.

583
00:42:19,070 --> 00:42:25,760
OK, so if I've got more than P threads ready to run,

584
00:42:25,830 --> 00:42:27,150
I execute any P.

585
00:42:27,220 --> 00:42:29,610
If I have fewer than P threads ready to run,

586
00:42:29,690 --> 00:42:31,470
we execute all of them.

587
00:42:31,530 --> 00:42:37,040
So, it turns out this is a good strategy.

588
00:42:37,120 --> 00:42:43,480
It's not a perfect strategy. In fact, the strategy of

589
00:42:43,560 --> 00:42:51,870
trying to schedule optimally a DAG on P processors

590
00:42:51,940 --> 00:42:55,480
is NP complete, meaning it's very difficult.

591
00:42:55,550 --> 00:42:59,460
So, those of you going to take 6.045 or 6.840,

592
00:42:59,540 --> 00:43:02,070
I highly recommend these courses,

593
00:43:02,150 --> 00:43:05,040
and we'll talk more about that in the last lectures

594
00:43:05,120 --> 00:43:06,280
we talked a little bit about

595
00:43:06,340 --> 00:43:11,140
what's coming up in the theory engineering concentration.

596
00:43:11,200 --> 00:43:13,900
You can learn about NP completeness

597
00:43:13,970 --> 00:43:15,710
and about how you show that certain problems,

598
00:43:15,780 --> 00:43:18,070
there are no good algorithms for them,

599
00:43:18,150 --> 00:43:19,730
OK, that we are aware of,

600
00:43:19,800 --> 00:43:22,120
OK, and what exactly that means.

601
00:43:22,200 --> 00:43:23,720
So, it turns out that

602
00:43:23,800 --> 00:43:26,820
this type of scheduling problem turns out

603
00:43:26,880 --> 00:43:29,940
to be a very difficult problem to get it optimal.

604
00:43:30,010 --> 00:43:34,750
But, there's nice theorem,

605
00:43:37,780 --> 00:43:43,350
due independently to Graham and Brent.

606
00:43:43,410 --> 00:43:46,800
It says, essentially,

607
00:43:46,860 --> 00:44:05,380
a greedy scheduler executes any computation G,

608
00:44:05,850 --> 00:44:21,730
with work, T_1, and critical path length T_∞

609
00:44:21,800 --> 00:44:33,360
in time T_P, less than or equal to T_1 over P plus T_∞

608
00:44:42,240 --> 00:44:45,510
on a computer with P processors.

609
00:44:45,590 --> 00:44:52,470
OK, so, it says that

612
00:44:52,550 --> 00:44:56,180
I can achieve T_1 over P plus T_∞

610
00:44:56,260 --> 00:44:58,750
So, what does that say?

611
00:44:58,820 --> 00:45:02,510
If we take a look and compare this

612
00:45:02,570 --> 00:45:07,090
with our lower bounds on runtime, how efficient is this?

613
00:45:07,170 --> 00:45:13,750
How does this compare with the optimal execution?

614
00:45:20,610 --> 00:45:23,950
Yeah, it's two competitive.

615
00:45:24,040 --> 00:45:26,820
It's within a factor of two of optimal

616
00:45:26,890 --> 00:45:29,160
because this is a lower bound

617
00:45:29,230 --> 00:45:32,550
and this is a lower bound.

618
00:45:32,630 --> 00:45:38,310
And so, if I take twice the max of these two,

619
00:45:38,390 --> 00:45:41,840
twice the maximum of these two,

620
00:45:41,910 --> 00:45:45,860
that's going to be bigger than the sum.

621
00:45:45,940 --> 00:45:47,690
So, I'm within a factor of two

622
00:45:47,750 --> 00:45:52,340
of which ever is the stronger lower bound for any situation.

623
00:45:52,410 --> 00:45:54,000
So, this says you get within a factor

624
00:45:54,130 --> 00:45:57,240
of two of efficiency of scheduling

625
00:45:57,310 --> 00:46:00,140
in terms of the runtime on P processors.

626
00:46:00,210 --> 00:46:02,020
OK, does everybody see that?

627
00:46:02,090 --> 00:46:04,270
So, let's prove this theorem.

628
00:46:04,340 --> 00:46:07,460
It's quite an elegant theorem.

629
00:46:07,540 --> 00:46:09,490
It's not a hard theorem. One of the nice things,

630
00:46:09,550 --> 00:46:11,740
by the way, about this week,

631
00:46:11,810 --> 00:46:13,870
is that nothing is very hard.

632
00:46:13,940 --> 00:46:16,450
It just requires you to think differently.

633
00:46:26,140 --> 00:46:29,930
OK, so the proof has to do with counting up

634
00:46:30,000 --> 00:46:32,860
how many complete steps we have,

635
00:46:32,950 --> 00:46:35,330
and how many incomplete steps we have.

636
00:46:35,400 --> 00:46:39,590
OK, so we'll start with the number of complete steps.

637
00:46:45,840 --> 00:46:50,280
So, can somebody tell me what's the largest number

638
00:46:50,360 --> 00:46:53,510
of complete steps I could possibly have?

639
00:46:57,040 --> 00:46:59,920
Yeah, I heard somebody mumble it back there.

640
00:47:02,580 --> 00:47:10,220
T_1 over P. Why is that?

641
00:47:15,690 --> 00:47:19,230
Yeah, so the number of complete steps is, at most,

642
00:47:19,300 --> 00:47:21,290
T_1 over P because why?

643
00:47:21,360 --> 00:47:25,760
[Student]：......

644
00:47:25,840 --> 00:47:30,150
Yeah, once you've had this many, you've done T_1 work, OK?

645
00:47:30,230 --> 00:47:33,710
So, every complete step I'm getting P work done.

646
00:47:33,790 --> 00:47:40,170
So, if I did more than T_1 over P steps,

647
00:47:40,230 --> 00:47:43,590
there would be no more work to be done.

648
00:47:43,660 --> 00:47:45,700
So, the number of complete steps

649
00:47:45,770 --> 00:47:49,000
can't be bigger than T_1 over P.

650
00:48:08,020 --> 00:48:13,350
OK, so that's this piece.

651
00:48:13,420 --> 00:48:19,030
OK, now we're going to count up the incomplete steps,

652
00:48:19,090 --> 00:48:21,540
and show its bounded by T infinity.

653
00:48:21,610 --> 00:48:24,380
OK, so let's consider an incomplete step.

654
00:48:27,530 --> 00:48:41,600
And, let's see what happens. And, let's let G prime be

655
00:48:41,680 --> 00:48:56,730
the subgraph of G that remains to be executed.

656
00:48:56,800 --> 00:49:01,910
OK, so we'll draw a picture here.

657
00:49:01,990 --> 00:49:04,170
So, imagine we have,

658
00:49:04,260 --> 00:49:06,060
let's draw it on a new board.

659
00:49:24,940 --> 00:49:29,870
So here, we're going to have a graph, our graph, G.

660
00:49:29,940 --> 00:49:31,130
We're going to do actually P

661
00:49:31,200 --> 00:49:32,780
equals three as our example here.

662
00:49:32,860 --> 00:49:35,640
So, imagine that this is the graph, G.

663
00:49:49,680 --> 00:49:53,610
And, I'm not showing the procedures here

664
00:49:53,690 --> 00:49:57,140
because this actually is a theorem that works for any DAG.

665
00:49:57,220 --> 00:50:01,340
And, the procedure outlines are not necessary.

666
00:50:01,420 --> 00:50:04,320
All we care about is the threads.

667
00:50:16,090 --> 00:50:28,640
I missed one. OK, so imagine that's my DAG, G

668
00:50:28,720 --> 00:50:33,560
and imagine that I have executed up to this point.

669
00:50:33,640 --> 00:50:38,230
Which ones have I executed?

670
00:50:45,270 --> 00:50:50,990
Yeah, I've executed these guys.

671
00:50:51,050 --> 00:50:57,400
So, the things that are in G prime are just the things

672
00:50:57,480 --> 00:50:59,060
that have yet to be executed.

673
00:50:59,140 --> 00:51:03,060
And these guys are the ones that are already executed.

674
00:51:06,800 --> 00:51:09,510
And, we'll imagine that all of them

675
00:51:09,570 --> 00:51:12,120
are unit time threads without loss of generality.

676
00:51:12,200 --> 00:51:14,820
The theorem would go through, even if each of these

677
00:51:14,890 --> 00:51:18,840
had a particular time associated with it.

678
00:51:18,910 --> 00:51:22,380
The same scheduling algorithm will work just fine.

679
00:51:22,460 --> 00:51:28,570
So, how can I characterize the threads

680
00:51:28,630 --> 00:51:30,050
that are ready to be executed?

681
00:51:30,130 --> 00:51:34,190
Which are the threads

682
00:51:34,270 --> 00:51:35,790
that are ready to be executed here?

683
00:51:35,860 --> 00:51:42,890
Let's just see. So, that one?

684
00:51:42,960 --> 00:51:46,570
No, that's not ready to be executed. Why?

685
00:51:46,640 --> 00:51:52,090
Because it's got a predecessor here, this guy.

686
00:51:52,160 --> 00:51:54,730
OK, so this guy is ready to be executed,

687
00:51:54,810 --> 00:51:57,540
and this guy is ready to be executed.

688
00:51:57,620 --> 00:52:04,350
OK, so those two threads are ready to be,

689
00:52:04,420 --> 00:52:05,810
how can I characterize this?

690
00:52:05,870 --> 00:52:06,980
What's their property?

691
00:52:07,040 --> 00:52:11,440
What's a graph theoretic property in G prime that tells me

692
00:52:11,510 --> 00:52:15,170
whether or not something is ready to be executed?

693
00:52:15,250 --> 00:52:17,740
It has no predecessor,

694
00:52:17,810 --> 00:52:20,280
but what's another way of saying that?

695
00:52:24,700 --> 00:52:28,070
It's got no predecessor in G prime.

696
00:52:28,150 --> 00:52:29,320
What does it mean for a node

697
00:52:29,400 --> 00:52:31,430
not to have a predecessor in a graph?

698
00:52:31,500 --> 00:52:38,350
Its in degree is zero, right? Same thing.

699
00:52:38,560 --> 00:52:55,290
So, the threads with in-degree zero in G prime

700
00:52:55,360 --> 00:52:58,880
are the ones that are ready to be executed.

701
00:53:01,690 --> 00:53:07,970
OK, and if it's incomplete step, what do I do?

702
00:53:08,030 --> 00:53:11,800
I'm going to execute says,

703
00:53:11,860 --> 00:53:14,100
if it's an incomplete step, I execute all of them.

704
00:53:14,180 --> 00:53:18,340
OK, so I execute all of these.

705
00:53:18,400 --> 00:53:24,270
OK, now when I execute all of the in-degree zero threads,

706
00:53:24,340 --> 00:53:31,940
what happens to the critical path length of the graph

707
00:53:32,010 --> 00:53:33,980
that remains to be executed?

708
00:53:34,060 --> 00:53:40,090
It decreases by one.

709
00:53:40,150 --> 00:53:53,600
So the critical path length of what remains to be executed

710
00:53:53,670 --> 00:53:58,570
of G prime, is reduced by one.

711
00:53:58,660 --> 00:54:05,020
So, what's left to be executed on every incomplete step,

712
00:54:05,090 --> 00:54:08,370
what's left to be executed always reduces by one.

713
00:54:08,450 --> 00:54:10,870
Notice the next step here is going to be a complete step,

714
00:54:10,940 --> 00:54:13,100
because I've got four things that are ready to go.

715
00:54:13,180 --> 00:54:16,790
And, I can execute them in such a way

716
00:54:16,870 --> 00:54:18,320
that the critical path length

717
00:54:18,380 --> 00:54:23,890
doesn't get reduced on that step.

718
00:54:23,960 --> 00:54:27,570
OK, but if I had to execute all of them,

719
00:54:27,650 --> 00:54:30,600
then it does reduce the critical path length.

720
00:54:30,660 --> 00:54:34,930
Now, of course, both could happen, OK, at the same time,

721
00:54:35,010 --> 00:54:37,800
OK, but any time that I have an incomplete step,

722
00:54:37,870 --> 00:54:41,710
I'm guaranteed to reduce the critical path length by one.

723
00:54:41,790 --> 00:54:53,290
OK, so that implies that the number of incomplete steps is,

724
00:54:53,360 --> 00:54:59,360
at most, T infinity. And so, therefore,

725
00:54:59,430 --> 00:55:07,280
T of P is, at most, the number of complete steps

726
00:55:07,340 --> 00:55:11,070
plus the number of incomplete steps.

727
00:55:11,160 --> 00:55:13,630
And we get our bound.

728
00:55:13,700 --> 00:55:15,750
This is sort of an amortized argument

729
00:55:15,820 --> 00:55:17,350
if you want to think of it that way,

730
00:55:17,420 --> 00:55:19,480
OK, that at every step

731
00:55:19,550 --> 00:55:22,140
I'm either amortizing the step against the work,

732
00:55:22,210 --> 00:55:26,470
or I'm amortizing it against the critical path length,

733
00:55:26,550 --> 00:55:27,940
or possibly both.

734
00:55:28,010 --> 00:55:31,850
But I'm at least doing one of those for every step,

735
00:55:31,950 --> 00:55:33,160
OK, and so, in the end,

736
00:55:33,230 --> 00:55:39,710
I just have to add up the two contributions.

737
00:55:39,790 --> 00:55:43,850
Any questions about that? So this, by the way,

738
00:55:43,930 --> 00:55:47,070
is the fundamental theorem of all scheduling.

739
00:55:47,130 --> 00:55:49,890
If ever you study anything having to do with scheduling,

740
00:55:49,960 --> 00:55:54,450
this basic result is sort of

741
00:55:54,530 --> 00:55:57,420
the foundation of a huge number of things.

742
00:55:57,490 --> 00:55:59,260
And then what people do is they gussy it up,

743
00:55:59,340 --> 00:56:03,540
like, let's do this online, OK, with a scheduler, etc.,

744
00:56:03,620 --> 00:56:06,450
that everybody's trying to match these bounds,

745
00:56:06,530 --> 00:56:11,210
OK, of what an omniscient greedy scheduler would achieve,

746
00:56:11,280 --> 00:56:15,750
OK, and there are all kinds of other things.

747
00:56:15,830 --> 00:56:19,940
But this is sort of the basic theorem

748
00:56:20,010 --> 00:56:22,690
that just pervades the whole area of scheduling.

749
00:56:22,760 --> 00:56:27,990
OK, let's do a quick corollary.

750
00:56:28,050 --> 00:56:31,090
I'm not going to erase those.

751
00:56:31,160 --> 00:56:37,850
Those are just too important. I want to erase those.

752
00:56:37,920 --> 00:56:41,440
Let's not erase those. I don't want to erase that either

753
00:56:41,450 --> 00:56:43,870
We can go back to the top.

754
00:56:43,950 --> 00:56:45,010
Actually, we'll put the corollary here

755
00:56:45,080 --> 00:56:53,260
because that's just one line. OK.

756
00:57:08,820 --> 00:57:12,330
The corollary says you get linear speed up

757
00:57:12,400 --> 00:57:17,420
if the number of processors that you allocate,

758
00:57:17,500 --> 00:57:25,340
that you run your job on is order, the parallelism.

759
00:57:27,590 --> 00:57:33,360
So greedy scheduler gives you linear speed up if you're

760
00:57:33,360 --> 00:57:40,790
running on essentially parallelism or fewer processors.

761
00:57:40,840 --> 00:57:44,120
OK, so let's see why that is.

762
00:57:44,180 --> 00:57:48,430
And I hope I'll fit this, OK?

763
00:57:48,500 --> 00:57:53,780
So, P bar is T_1 over T infinity.

764
00:57:53,860 --> 00:57:58,530
And that implies that

765
00:57:58,610 --> 00:58:04,720
if P equals order T_1 over T infinity,

766
00:58:04,790 --> 00:58:10,880
then that says just bringing those around,

767
00:58:10,960 --> 00:58:21,410
T infinity is order T_1 over P. So, everybody with me?

768
00:58:21,470 --> 00:58:23,030
It's just algebra.

769
00:58:23,100 --> 00:58:29,220
So, it says this is the definition of parallelism,

770
00:58:29,300 --> 00:58:32,230
T_1 over T infinity, and so,

771
00:58:32,300 --> 00:58:35,130
if P is order parallelism,

772
00:58:35,200 --> 00:58:37,720
then it's order T_1 over T infinity.

773
00:58:37,800 --> 00:58:39,610
And now, just bring it around.

774
00:58:39,670 --> 00:58:41,450
It says T infinity is order T_1 over P.

775
00:58:41,530 --> 00:58:46,130
So, that says T infinity is order T_1 over P.

776
00:58:46,210 --> 00:58:57,450
OK, and so, therefore, continue the proof here,

777
00:58:57,520 --> 00:59:08,650
thus T_P is at most T_1 over P plus T infinity.

778
00:59:08,730 --> 00:59:13,270
Well, if this is order T_1 over P,

779
00:59:13,340 --> 00:59:16,500
the whole thing is order T_1 over P.

780
00:59:16,570 --> 00:59:30,260
OK, and so, now I have T_P is order T_1 over P,

781
00:59:30,340 --> 00:59:37,320
and what we need is to compute T_1 over T_P,

782
00:59:37,400 --> 00:59:41,160
and that's going to be order T_p. Ok?

783
00:59:45,600 --> 00:59:47,290
Does everybody see that?

784
00:59:47,360 --> 00:59:48,500
So what that says is that

785
00:59:48,570 --> 00:59:50,520
if I have a certain amount of parallelism,

786
00:59:50,600 --> 00:59:52,800
if I run essentially on

787
00:59:52,870 --> 00:59:55,480
fewer processors than that parallelism,

788
00:59:55,540 --> 00:59:58,590
I get linear speed up if I use greedy scheduling.

789
00:59:58,660 --> 01:00:04,250
OK, if I run on more processors than the parallelism,

790
01:00:04,320 --> 01:00:06,600
in some sense I'm being wasteful

791
01:00:06,680 --> 01:00:08,990
because I can't possibly get enough speed up

792
01:00:09,070 --> 01:00:11,530
to justify those extra processors.

793
01:00:11,590 --> 01:00:14,830
So, understanding parallelism of a job says

794
01:00:14,890 --> 01:00:16,700
that's sort of a limit

795
01:00:16,770 --> 01:00:18,550
on the number of processors I want to have.

796
01:00:18,610 --> 01:00:19,840
And, in fact, I can achieve that.

797
01:00:19,920 --> 01:00:21,330
Question?

798
01:00:21,400 --> 01:00:38,120
[Student]：......

799
01:00:38,190 --> 01:00:39,470
[Prof]：Yeah, really, in some sense,

800
01:00:39,540 --> 01:00:41,690
this is saying it should be omega P.

801
01:00:41,760 --> 01:00:43,310
Yeah, so that's fine.

802
01:00:43,380 --> 01:00:46,710
It's a question of, so ask again.

803
01:00:46,780 --> 01:01:02,140
[Student]：......

804
01:01:02,210 --> 01:01:04,630
[Prof]：No, it's only if it's bounded above by a constant.

805
01:01:04,690 --> 01:01:07,390
T_1 and T infinity aren't constants.

806
01:01:07,460 --> 01:01:11,460
They're variables in this.

807
01:01:11,530 --> 01:01:17,080
So, we are doing multivariable asymptotic analysis.

808
01:01:17,150 --> 01:01:18,880
So, any of these things can be

809
01:01:18,950 --> 01:01:20,100
a function of anything else,

810
01:01:20,170 --> 01:01:22,950
and can be growing as much as we want.

811
01:01:23,030 --> 01:01:23,730
So, the fact that we say

812
01:01:23,770 --> 01:01:25,060
we are given it for a particular thing,

813
01:01:25,130 --> 01:01:27,090
we're really not given that number.

814
01:01:27,160 --> 01:01:29,560
We're given a whole class of DAG's

815
01:01:29,630 --> 01:01:31,370
or whatever of various sizes

816
01:01:31,450 --> 01:01:33,220
is really what we're talking about.

817
01:01:33,280 --> 01:01:34,980
So, I can look at the growth.

818
01:01:35,050 --> 01:01:36,370
Here, where it's talking about

819
01:01:36,440 --> 01:01:42,450
the growth of the parallelism,

820
01:01:42,510 --> 01:01:48,910
sorry, the growth of the runtime T_P

821
01:01:48,990 --> 01:01:52,260
as a function of T_1 and T infinity.

822
01:01:52,320 --> 01:01:57,870
So, I am talking about things that are growing here, OK?

823
01:02:00,230 --> 01:02:08,000
OK, so let's put this to work, OK?

824
01:02:08,070 --> 01:02:12,080
And, in fact, so now I'm going to go back to here.

825
01:02:12,150 --> 01:02:14,360
Now I'm going to tell you about

826
01:02:14,430 --> 01:02:15,980
a little bit of my own research,

827
01:02:16,060 --> 01:02:21,570
and how we use this in some of the work that we did.

828
01:02:21,650 --> 01:02:28,160
OK, so we've developed a dynamic multithreaded language

829
01:02:28,220 --> 01:02:30,280
called Cilk,  spelled with a C

830
01:02:30,360 --> 01:02:32,390
because it's based on the language, C.

831
01:02:32,460 --> 01:02:34,540
And, it's not an acronym

832
01:02:34,620 --> 01:02:39,030
because cilk is like nice threads,

833
01:02:39,120 --> 01:02:47,590
OK, although at one point my students had a competition

834
01:02:47,660 --> 01:02:52,760
for what the acronym cilk could mean.

835
01:02:52,830 --> 01:02:56,320
The winner, turns out,

836
01:02:56,400 --> 01:03:00,100
was Charles' Idiotic Linguistic Kluge.

837
01:03:00,170 --> 01:03:06,000
So anyway, if you want to take a look at it,

838
01:03:06,080 --> 01:03:12,450
you can find some stuff on it here.

839
01:03:17,760 --> 01:03:21,890
OK, and what it uses is actually

840
01:03:21,960 --> 01:03:24,010
one of these more complicated schedulers.

841
01:03:24,090 --> 01:03:29,950
It's a randomized online scheduler,

842
01:03:33,730 --> 01:03:36,930
OK, and if you look at

843
01:03:37,000 --> 01:03:41,530
its expected runtime on P processors,

844
01:03:41,600 --> 01:03:43,820
it gets effectively

845
01:03:43,880 --> 01:03:50,620
T_1 over P plus O of T infinity provably.

846
01:03:57,380 --> 01:03:59,820
OK, and empirically,

847
01:03:59,890 --> 01:04:03,280
if you actually look at what kind of runtimes you get

848
01:04:03,350 --> 01:04:05,580
to find out what's hidden in the big O there,

849
01:04:05,650 --> 01:04:10,500
it turns out, in fact, it's T_1 over P plus T infinity

850
01:04:10,570 --> 01:04:14,420
with the constants here being very close to one empirically

851
01:04:16,640 --> 01:04:18,300
So, no guarantees,

852
01:04:18,370 --> 01:04:21,460
but this turns out to be a pretty good bound.

853
01:04:21,540 --> 01:04:24,140
Sometimes, you see a coefficient on T infinity

854
01:04:24,210 --> 01:04:27,290
that's up maybe close to four or something.

855
01:04:27,360 --> 01:04:29,140
But generally, you don't see something

856
01:04:29,210 --> 01:04:30,950
that's much bigger than that.

857
01:04:31,010 --> 01:04:32,660
And mostly, it tends to be around,

858
01:04:32,730 --> 01:04:35,980
if you do a linear regression curve fit,

859
01:04:36,050 --> 01:04:38,550
you get that the constant here is close to one.

860
01:04:38,630 --> 01:04:42,430
And so, with this,

861
01:04:42,500 --> 01:04:49,010
you get near perfect if you use this formula

862
01:04:49,080 --> 01:04:50,650
as a model for your runtime.

863
01:04:50,710 --> 01:04:53,820
You get near perfect linear speed up

864
01:04:56,710 --> 01:05:02,090
if the number of processors you're running on is

865
01:05:02,160 --> 01:05:05,010
much less than your average parallelism,

866
01:05:05,080 --> 01:05:06,560
which, of course,

867
01:05:06,640 --> 01:05:11,220
is the same thing as if T infinity

868
01:05:11,290 --> 01:05:14,950
is much less than T_1 over P.

869
01:05:15,020 --> 01:05:17,970
So, what happens here is that

870
01:05:18,060 --> 01:05:22,030
when P is much less than P infinity, that is,

871
01:05:22,100 --> 01:05:24,420
T infinity is much less than T_1 over P,

872
01:05:24,490 --> 01:05:27,930
this term ceases to matter very much,

873
01:05:28,010 --> 01:05:31,070
and you get very good speedup,

874
01:05:31,130 --> 01:05:34,000
OK, in fact, almost perfect speedup.

875
01:05:34,090 --> 01:05:37,540
So, each processor gives you another processor's work

876
01:05:37,610 --> 01:05:39,440
as long as you are the range

877
01:05:39,500 --> 01:05:41,030
where the number of processors

878
01:05:41,100 --> 01:05:46,230
is much less than the number of parallelism.

879
01:05:46,310 --> 01:05:51,450
Now, with this language many years ago,

880
01:05:51,530 --> 01:05:53,790
which seems now like many years ago,

881
01:05:53,850 --> 01:05:57,660
OK, it turned out we competed.

882
01:05:57,730 --> 01:06:03,080
We built a bunch of chess programs.

883
01:06:05,270 --> 01:06:16,330
And, among our programs were Starsocrates and Cilkchess

884
01:06:16,410 --> 01:06:19,060
and we also had several others.

885
01:06:19,140 --> 01:06:22,890
And these were, I would call them, world-class.

886
01:06:22,980 --> 01:06:25,490
In particular, we tied for first

887
01:06:25,550 --> 01:06:31,050
in the 1995 World Computer Chess Championship in Hong Kong,

888
01:06:31,120 --> 01:06:34,280
and then we had a playoff and we lost.

889
01:06:34,350 --> 01:06:35,990
It was really a shame.

890
01:06:36,070 --> 01:06:39,260
We almost won, running on a big parallel machine.

891
01:06:39,340 --> 01:06:42,520
That was, incidentally, some of you may know

892
01:06:42,600 --> 01:06:45,920
about the Deep Blue chess playing program.

893
01:06:45,990 --> 01:06:47,860
That was the last time

894
01:06:47,930 --> 01:06:53,750
before they faced then world champion Kasparov

895
01:06:53,810 --> 01:06:56,350
that they competed against programs.

896
01:06:56,410 --> 01:06:58,470
They tied for third in that tournament.

897
01:06:58,550 --> 01:07:01,780
OK, so we actually out-placed them.

898
01:07:01,860 --> 01:07:03,830
However, in the head-to-head competition,

899
01:07:03,890 --> 01:07:05,480
we lost to them.

900
01:07:05,560 --> 01:07:07,860
So we had one loss in the tournament

901
01:07:07,930 --> 01:07:10,190
up to the point of the finals.

902
01:07:10,270 --> 01:07:12,380
They had a loss and a draw.

903
01:07:12,470 --> 01:07:15,100
Most people aren't aware that Deep Blue, in fact,

904
01:07:15,180 --> 01:07:17,290
was not the reigning World Computer Chess Championship

905
01:07:17,360 --> 01:07:21,660
when they faced Kasparov.

906
01:07:21,730 --> 01:07:25,340
The reason that they faced Kasparov

907
01:07:25,400 --> 01:07:27,310
was because IBM was willing to put up the money.

908
01:07:27,370 --> 01:07:33,660
OK, so we developed these chess programs,

909
01:07:33,730 --> 01:07:37,250
and the way we developed them,

910
01:07:37,330 --> 01:07:41,580
let me in particular talk about Starsocrates.

911
01:07:41,640 --> 01:07:45,490
We had this interesting anomaly come up.

912
01:07:45,550 --> 01:07:51,440
We were running on

913
01:07:51,510 --> 01:08:01,740
a 32 processor computer at MIT for development.

914
01:08:01,800 --> 01:08:05,950
And, we had access to a 512 processor computer

915
01:08:06,030 --> 01:08:19,570
for the tournament at NCSA at the University of Illinois.

916
01:08:19,650 --> 01:08:21,300
So, we had this big machine.

917
01:08:21,370 --> 01:08:22,820
Of course, they didn't want to give it to us very much,

918
01:08:22,890 --> 01:08:25,700
but we have the same machine, just a small one, at MIT.

919
01:08:25,770 --> 01:08:28,210
So, we would develop on this,

920
01:08:28,290 --> 01:08:30,300
and occasionally we'd be able to run on this,

921
01:08:30,360 --> 01:08:33,530
and this was what we were developing for on our processor.

922
01:08:33,600 --> 01:08:39,120
So, let me show you sort of the anomaly that came up, OK?

923
01:08:47,060 --> 01:08:50,520
So, we had a version of a program

924
01:08:50,580 --> 01:08:52,410
that I'll call the original program,

925
01:08:52,480 --> 01:09:01,440
OK, and we had an optimized program

926
01:09:01,520 --> 01:09:09,860
that included some new features

927
01:09:09,940 --> 01:09:12,660
that were supposed to make the program go faster.

928
01:09:12,740 --> 01:09:24,200
And so, we timed it on our 32 processor machine.

929
01:09:24,280 --> 01:09:27,230
And, it took us 65 seconds to run it.

930
01:09:27,310 --> 01:09:34,140
OK, and then we timed this new program.

931
01:09:34,220 --> 01:09:37,050
So, I'll call that T prime of sub 32

932
01:09:37,050 --> 01:09:40,730
on our 32 processor machine,

933
01:09:40,810 --> 01:09:42,800
and it ran and 40 seconds

934
01:09:42,870 --> 01:09:46,800
to do this particular benchmark.

935
01:09:46,880 --> 01:09:50,090
Now, let me just say, I've lied about

936
01:09:50,160 --> 01:09:53,470
the actual numbers here to make the calculations easy.

937
01:09:53,540 --> 01:09:55,330
But, the same idea happened.

938
01:09:55,400 --> 01:09:56,920
Just the numbers were messier.

939
01:09:56,990 --> 01:10:01,450
OK, so this looks like

940
01:10:01,530 --> 01:10:04,940
a significant improvement in runtime,

941
01:10:05,010 --> 01:10:08,690
but we rejected the optimization.

942
01:10:17,810 --> 01:10:21,770
OK, and the reason we rejected it

943
01:10:21,840 --> 01:10:23,300
is because we understood

944
01:10:23,380 --> 01:10:26,300
about the issues of work and critical path.

945
01:10:26,360 --> 01:10:28,390
So, let me show you the analysis that we did,

946
01:10:28,470 --> 01:10:31,290
OK? So the analysis,

947
01:10:31,370 --> 01:10:33,300
it turns out, if we looked at our instrumentation,

948
01:10:33,370 --> 01:10:36,730
the work in this case was 2,048.

949
01:10:36,810 --> 01:10:42,090
And, the critical path was one second,

950
01:10:42,170 --> 01:10:45,630
which, over here with the optimized program,

951
01:10:45,700 --> 01:10:49,640
the work was, in fact, 1,024.

952
01:10:49,730 --> 01:10:54,960
But the critical path was eight.

953
01:10:55,020 --> 01:11:00,350
So, if we plug into our simple model here,

954
01:11:00,420 --> 01:11:05,060
the one I have up there with the approximation there,

955
01:11:05,140 --> 01:11:17,670
I have T_32 is equal to T_1 over 32 plus T infinity,

956
01:11:17,740 --> 01:11:20,070
and that's equal to,

957
01:11:20,130 --> 01:11:24,560
well, the work is 2,048 divided by 32.

958
01:11:24,640 --> 01:11:25,790
What's that?

959
01:11:25,860 --> 01:11:32,400
64, good, plus the critical path, one, that's 65.

960
01:11:32,470 --> 01:11:34,760
So, that checks out with what we saw.

961
01:11:34,830 --> 01:11:36,740
OK, in fact, we did that,

962
01:11:36,820 --> 01:11:37,970
and it checked out.

963
01:11:38,030 --> 01:11:40,360
OK, it was very close. OK, over here,

964
01:11:40,440 --> 01:11:51,540
T prime of 32 is T prime one over 32,

965
01:11:51,600 --> 01:11:54,920
plus T infinity prime,

966
01:11:54,980 --> 01:12:02,230
and that's equal to 1,024 divided by 32 is 32

967
01:12:02,300 --> 01:12:07,030
plus eight, the critical path here. That's 40.

968
01:12:07,100 --> 01:12:09,150
So, that checked out too.

969
01:12:09,220 --> 01:12:11,170
So, now what we did is we said

970
01:12:11,240 --> 01:12:14,060
OK, let's extrapolate to our big machine.

971
01:12:14,130 --> 01:12:17,020
How fast are these things going to run on our big machine?

972
01:12:17,080 --> 01:12:23,660
Well, for that, we want T of 512.

973
01:12:23,720 --> 01:12:29,910
And, that's equal to T_1 over 512 plus T infinity.

974
01:12:29,990 --> 01:12:35,150
And so, what's 2,048 divided by 512?

975
01:12:35,230 --> 01:12:38,950
It's four, plus T infinity is one.

976
01:12:39,010 --> 01:12:40,900
That's equal to five.

977
01:12:40,970 --> 01:12:43,740
So, go quite a bit faster on this.

978
01:12:43,800 --> 01:12:54,250
But here, T prime of 512 is equal to T one prime over 512

979
01:12:54,330 --> 01:12:57,270
plus T infinity prime is equal to,

980
01:12:57,350 --> 01:13:03,320
well, 1,024 plus divided by 512

981
01:13:03,390 --> 01:13:07,220
is two plus critical path of eight, that's ten.

982
01:13:08,950 --> 01:13:13,260
OK, and so, you see that on the big machine,

983
01:13:13,330 --> 01:13:15,250
we would have been running

984
01:13:15,330 --> 01:13:18,550
twice as slow had we adopted that,

985
01:13:18,620 --> 01:13:23,550
quote, "Optimization",

986
01:13:23,620 --> 01:13:27,310
OK, because we had run out of parallelism,

987
01:13:27,370 --> 01:13:32,690
and this was making the critical path longer.

988
01:13:32,750 --> 01:13:34,610
We needed to have a way of doing it

989
01:13:34,690 --> 01:13:36,200
where we could reduce the work.

990
01:13:36,270 --> 01:13:37,630
Yeah, it's good to reduce the work

991
01:13:37,700 --> 01:13:39,370
but not if the critical path

992
01:13:39,450 --> 01:13:41,650
ends up getting rid of the parallelism

993
01:13:41,730 --> 01:13:43,970
that we hope to be able to use during the runtime.

994
01:13:44,040 --> 01:13:47,730
So, it's twice as slow, OK, twice as slow.

995
01:13:47,800 --> 01:13:51,810
So the moral is that the work and critical path length

996
01:13:51,880 --> 01:13:55,510
predict the performance better than the execution time alone

997
01:13:55,580 --> 01:13:58,170
OK, when you look at scalability.

998
01:13:58,240 --> 01:14:00,880
And a big issue on a lot of these machines is scalability;

999
01:14:00,950 --> 01:14:03,830
not always, sometimes you're not worried about scalability.

1000
01:14:03,910 --> 01:14:04,880
Sometimes you just care.

1001
01:14:04,950 --> 01:14:06,860
Had we been running in the competition

1002
01:14:06,930 --> 01:14:09,440
on a 32 processor machine,

1003
01:14:09,510 --> 01:14:11,050
we would have accepted this optimization.

1004
01:14:11,130 --> 01:14:13,230
It would have been a good trade-off.

1005
01:14:13,310 --> 01:14:17,000
OK, but because we knew that we were

1006
01:14:17,070 --> 01:14:19,970
running on a machine with a lot more processors,

1007
01:14:20,060 --> 01:14:22,200
and that we were close to running out of the parallelism,

1008
01:14:22,270 --> 01:14:23,410
it didn't make sense to be

1009
01:14:23,480 --> 01:14:26,010
increasing the critical path at that point,

1010
01:14:26,080 --> 01:14:27,640
because that was just reducing

1011
01:14:27,710 --> 01:14:32,150
the parallelism of our calculation.

1012
01:14:32,230 --> 01:14:36,910
OK, next time,

1013
01:14:36,990 --> 01:14:40,190
any questions about that first? No?

1014
01:14:40,270 --> 01:14:42,360
OK. Next time, now that we understand

1015
01:14:42,430 --> 01:14:45,530
the model for execution,

1016
01:14:45,600 --> 01:14:47,640
we're going to start looking at

1017
01:14:47,720 --> 01:14:50,600
the performance of particular algorithms

1018
01:14:50,660 --> 01:14:54,980
what we code them up in a dynamic, multithreaded style,

1019
01:04:45,580 --> 01:04:46,470
[student]Inaudible

